{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8yW0a5OlyIx",
        "outputId": "68b47be6-4e2c-4e49-bc4b-9d5d49a9c7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles for Dawood Ibrahim...\n",
            "Failed to fetch article content from https://www.hindustantimes.com/india-news/honoured-to-be-his-in-law-javed-miandad-praises-dawood-ibrahim-101710894987676.html: 401 Client Error: Unauthorized for url: https://www.hindustantimes.com/india-news/honoured-to-be-his-in-law-javed-miandad-praises-dawood-ibrahim-101710894987676.html\n",
            "Fetched 5 articles for Dawood Ibrahim\n",
            "Articles saved to all_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import time\n",
        "\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = ' '.join([para.get_text() for para in paragraphs])\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch article content from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def fetch_articles(query, num_articles=5):\n",
        "    articles = []\n",
        "    start = 0\n",
        "    while len(articles) < num_articles:\n",
        "        url = f'https://www.google.com/search?q={urllib.parse.quote_plus(query)}&tbm=nws&start={start}'\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for item in soup.find_all('a'):\n",
        "            link = item.get('href')\n",
        "            if link and '/url?q=' in link:\n",
        "                title_element = item.find('div', class_='BNeawe vvjwJb AP7Wnd')\n",
        "                snippet_element = item.find('div', class_='BNeawe s3v9rd AP7Wnd')  # Snippet element\n",
        "                if title_element and snippet_element:\n",
        "                    title = title_element.get_text()\n",
        "                    snippet = snippet_element.get_text()\n",
        "                    link = link.split('/url?q=')[1].split('&')[0]\n",
        "                    full_link = urllib.parse.unquote(link)\n",
        "                    content = fetch_article_content(full_link)\n",
        "                    articles.append({\n",
        "                        'title': title,\n",
        "                        'link': full_link,\n",
        "                        'snippet': snippet,\n",
        "                        'content': content,\n",
        "                        'personality': query\n",
        "                    })\n",
        "                    if len(articles) >= num_articles:\n",
        "                        break\n",
        "        start += 10\n",
        "        time.sleep(1)  # Avoid hitting Google too frequently\n",
        "    return articles\n",
        "\n",
        "def fetch_and_save_articles(personalities, num_articles=5):\n",
        "    all_articles = []\n",
        "    for personality in personalities:\n",
        "        print(f\"Fetching articles for {personality}...\")\n",
        "        articles = fetch_articles(personality, num_articles)\n",
        "        all_articles.extend(articles)\n",
        "        print(f\"Fetched {len(articles)} articles for {personality}\")\n",
        "\n",
        "    # Save all articles to a single CSV file\n",
        "    df = pd.DataFrame(all_articles)\n",
        "    df.to_csv('all_articles.csv', index=False)\n",
        "    print(\"Articles saved to all_articles.csv\")\n",
        "\n",
        "personalities = [\"Dawood Ibrahim\"]\n",
        "\n",
        "fetch_and_save_articles(personalities)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('all_articles.csv')\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def analyze_sentiment(text):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        sentiment = analyzer.polarity_scores(text)\n",
        "        return 'negative' if sentiment['neg'] > 0.5 else 'positive'\n",
        "    return 'neutral'  # Handle non-string values\n",
        "\n",
        "# Convert all entries in 'content' to strings and handle missing values\n",
        "df['content'] = df['content'].fillna('').astype(str)\n",
        "\n",
        "# Apply the sentiment analysis\n",
        "df['sentiment'] = df['content'].apply(analyze_sentiment)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Use a basic count vectorizer and Naive Bayes classifier for training\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the model\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model\n",
        "import joblib\n",
        "joblib.dump(model, 'sentiment_analysis_model.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBXT4Fs-mc_y",
        "outputId": "ac03a97a-5aa8-43e3-f4bd-3f671dc281a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Model and vectorizer saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import joblib\n",
        "\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = ' '.join([para.get_text() for para in paragraphs])\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch article content from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def fetch_articles(query, num_articles=5):\n",
        "    articles = []\n",
        "    start = 0\n",
        "    while len(articles) < num_articles:\n",
        "        url = f'https://www.google.com/search?q={urllib.parse.quote_plus(query)}&tbm=nws&start={start}'\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for item in soup.find_all('a'):\n",
        "            link = item.get('href')\n",
        "            if link and '/url?q=' in link:\n",
        "                title_element = item.find('div', class_='BNeawe vvjwJb AP7Wnd')\n",
        "                snippet_element = item.find('div', class_='BNeawe s3v9rd AP7Wnd')  # Snippet element\n",
        "                if title_element and snippet_element:\n",
        "                    title = title_element.get_text()\n",
        "                    snippet = snippet_element.get_text()\n",
        "                    link = link.split('/url?q=')[1].split('&')[0]\n",
        "                    full_link = urllib.parse.unquote(link)\n",
        "                    content = fetch_article_content(full_link)\n",
        "                    articles.append({\n",
        "                        'title': title,\n",
        "                        'link': full_link,\n",
        "                        'snippet': snippet,\n",
        "                        'content': content,\n",
        "                        'personality': query\n",
        "                    })\n",
        "                    if len(articles) >= num_articles:\n",
        "                        break\n",
        "        start += 10\n",
        "    return articles\n",
        "\n",
        "def fetch_and_save_new_articles(personality, num_articles=5):\n",
        "    print(f\"Fetching articles for {personality}...\")\n",
        "    articles = fetch_articles(personality, num_articles)\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('new_articles.csv', index=False)\n",
        "    print(\"New articles saved to new_articles.csv\")\n",
        "\n",
        "def predict_sentiment_for_new_articles():\n",
        "    vectorizer = joblib.load('vectorizer.joblib')\n",
        "    model = joblib.load('sentiment_model.joblib')\n",
        "\n",
        "    def predict_sentiment(article_content):\n",
        "        content_vectorized = vectorizer.transform([article_content])\n",
        "        prediction = model.predict(content_vectorized)\n",
        "        return prediction[0]\n",
        "\n",
        "    new_articles = pd.read_csv('new_articles.csv')\n",
        "    new_articles['predicted_sentiment'] = new_articles['content'].apply(predict_sentiment)\n",
        "    new_articles.to_csv('new_articles_with_sentiment.csv', index=False)\n",
        "    print(\"Sentiment predictions saved to new_articles_with_sentiment.csv\")\n",
        "\n",
        "# Fetch new articles\n",
        "personality = \"Pablo Escobar\"  # Replace with the desired personality name\n",
        "fetch_and_save_new_articles(personality)\n",
        "\n",
        "# Predict sentiment for the new articles\n",
        "predict_sentiment_for_new_articles()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdYjYBmMnTaL",
        "outputId": "0df50e10-414a-467b-ad9d-f3eb7823f678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles for Pablo Escobar...\n",
            "New articles saved to new_articles.csv\n",
            "Sentiment predictions saved to new_articles_with_sentiment.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the vectorizer and the model\n",
        "vectorizer = joblib.load('vectorizer.joblib')\n",
        "model = joblib.load('sentiment_model.joblib')\n",
        "\n",
        "# Function to predict sentiment of new articles\n",
        "def predict_sentiment(article_content):\n",
        "    content_vectorized = vectorizer.transform([article_content])\n",
        "    prediction = model.predict(content_vectorized)\n",
        "    return prediction[0]\n",
        "\n",
        "# Example usage\n",
        "new_articles = pd.read_csv('new_articles.csv')  # Assuming you have new articles in a CSV file\n",
        "new_articles['predicted_sentiment'] = new_articles['content'].apply(predict_sentiment)\n",
        "print(new_articles[['title', 'predicted_sentiment']])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylCzGADYndal",
        "outputId": "38d391ba-95d7-43a0-d5c2-9f5a7700d5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title predicted_sentiment\n",
            "0             What Happened To Pablo Escobar's Body?            positive\n",
            "1             What Happened To Pablo Escobar's Body?            positive\n",
            "2             What Happened To Pablo Escobar's Body?            positive\n",
            "3             What Happened To Pablo Escobar's Body?            positive\n",
            "4  Talking Animals: Journalist Hammer Discusses R...            positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "Wp_dcRUFozKU",
        "outputId": "a6988774-7c2d-4244-adcb-4f1170ca2fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sentiment'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7cf8833f280d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Use the vectorizer and model from the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
          ]
        }
      ]
    }
  ]
}